[
  {
    "text": "41\n오픈소스 모델 고려사항\n오픈소스 모델을 활용하는 사업자는 직접 모델을 개발하는 사업자와 아래와 같은 차이가 있다. 인공지능 이용에 따라 \n직접 영향을 받지 않는 \n이용자\n영역\n(B\n)\n법상 의미를 벗어나지만 실제 안전하고 신뢰할만한 인공지능을 위해 고려해야 하는 \n영역\n예를 들어 핵물질과 원자력시설의 안전한 관리 및 운영\n등에 인공지능이 사용\n. ‘\n’ \n될 경우\n인공지능 관련 위험 발생 시 이용자보다는 일반 국민이 영향을 받을 가능성\n, \n이 높음\n이 경우\n이용자 보호방안은 영향받는 자를 고려하여 설계되어야 함\n. , \n‘\n’\n. 소주제 번호\n직접 개발\n오픈소스 모델\n3-1-1\n수집 단계부터 데이터의 투명성\n품질\n적법성을 \n, \n, \n통제할 수 있음. 모델 원저작자가 배포한 데이터 명세서가 있는\n지 파악하고\n파인튜닝 시에는 적법한 고품질 \n, \n데이터를 사용하여 오픈소스 커뮤니티 등에서 \n알려진 위험으로부터 이용자를 보호하는 것이 \n바람직함. 3-1-2\n이용자 보호를 위해 아키텍처 설계 단계부터 레\n드팀 \n을 투입하여 보안 취약점\n(Red-teaming)\n을 사전에 차단할 수 있음. 모델의 가중치 파일에 악성 코드가 포함되어 있\n거나 특정 입력값에 오작동하도록 설계된 백도\n어 공격에 취약할 수 있으므로\n신뢰할 수 있는 \n, \n저장소에서 다운로드하고 파일 해시값 검증 및 \n보안 스캔을 수행하는 것이 필요. 3-1-3\n인공지능 수명주기에 걸쳐 자체적인 테스트 환\n경에서 검증 수행이 가능. 모델 원저작자가 발표한 벤치마크 점수는 일반\n적인 상황에서의 성능이므로\n사업자의 제품서\n, \n·\n비스 환경에서는 성능이 급격히 저하될 수 있\n음\n자체적인 시험평가 결과를 확보하는 것이 \n. 필요. 3-2-1\n인공지능 내부 로그와 연동하여 세밀한 모니터\n링 체계를 구축할 수 있음. 입출력을 검증하는 가드레일 시스템을 별도로 \n구축하여 모니터링을 수행할 수 있음. 3-2-2\n피드백 반영을 위해 인공지능 업데이트 시\n모\n, \n델 아키텍처나 하이퍼파라미터를 직접 수정하여 \n근본적인 개선이 가능. 수렴된 피드백을 분석하여 업데이트 조치로 연\n계하되\n모델 자체의 결함인 경우 프롬프트 엔\n, \n지니어링이나 추가 튜닝을 통해 보완 구조를 마\n련하는 것이 바람직함. 3-2-3\n시스템 전체에 대한 통제권을 바탕으로 이용자\n의 설명 요구 등에 상세히 대응이 가능. 이용자에게 해당 제품서비스가 특정 오픈소스 \n·\n모델에 기반하여 운용된다는 사실을 사전에 고\n지하는 것이 바람직함.",
    "metadata": {
      "page": 42,
      "source": "4._260122_고영향_인공지능_사업자_책무_가이드라인-1.pdf",
      "chunk_index": 0,
      "chunk_method": "semantic_local"
    }
  },
  {
    "text": "",
    "metadata": {
      "page": 42,
      "source": "4._260122_고영향_인공지능_사업자_책무_가이드라인-1.pdf",
      "chunk_index": 1,
      "chunk_method": "semantic_local"
    }
  }
]