[
  {
    "text": "44\n사례\n인공지능 모델 공격 방어를 위한 모델 최적화 방안\n방어 대책\n설명 및 기법 예시\n데이터 유형\nDefensive \nDistillation\n복잡한 신경망의 지식을 간단한 신경망으로 전이시키는 방법\n원본 모델의 확률 \n. 분포를 얻어 증류\n모델을 훈련하면\n증류 모델은 원본 모델의 특성을 \n(distillation) \n, \n보전하게 됨\n작업 수행 시 증류 모델을 활용하면 적대적 공격에 대응 가능\n. . 이미지\n오디오\nGradient \nRegularization\n대부분의 적대적 공격은 모델 추론 과정에서의 경사\n를 보고 공격이 이\n(gradient)\n루어지므로 모델의 경사가 출력으로 노출되는 것을 방지하는 것에 중점을 둠. 모델의 경사를 일관된 형태로 유지예\n- Gradient Regularization: \n(\n: Bit Plane \nFeature Consistency (BPFC) regularizer, Second-Order Adversarial \nRegularizer (SOAR))\n출력에 노이즈를 추가하거나\n학습 중에 특정 부분을 제\n- Gradient masking: \n, \n거함으로써 모델의 경사를 외부로부터 감춤예\n(\n: S2SNet)\n이미지\n오디오\n텍스트\nGradient \nMasking\nStochastic \nNetwork\n학습 모델의 불확실성을 다루기 위한 확률적인 요소를 도입하는 네트워크\n이를 \n. 통해 모델의 결정을 불확실하게 만들어 적대적 사례에 대한 저항성을 향상 예\n(\n: \ndefensive dropout, Random Self-Ensemble (RSE)).",
    "metadata": {
      "page": 45,
      "source": "4._260122_고영향_인공지능_사업자_책무_가이드라인-1.pdf",
      "chunk_index": 0,
      "chunk_method": "semantic_local"
    }
  },
  {
    "text": "",
    "metadata": {
      "page": 45,
      "source": "4._260122_고영향_인공지능_사업자_책무_가이드라인-1.pdf",
      "chunk_index": 1,
      "chunk_method": "semantic_local"
    }
  }
]